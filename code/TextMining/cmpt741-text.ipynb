{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import evaluate, print_perf \n",
    "from surprise import accuracy\n",
    "from surprise import GridSearch\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/ting/Documents/CMPT741/project/train_rating.txt')\n",
    "\n",
    "cols = ['train_id','user_id', 'business_id', 'rating']\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "review_input = '/Users/ting/Documents/CMPT741/project/train_review.json'\n",
    "df_review = pd.read_json(review_input,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2352.5451061725616 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# df_review0 = df_review.head(100000)\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# en_stop = get_stop_words('en')\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "# for index, row in df_review0.iterrows():\n",
    "#     #print(index)\n",
    "#     text = row['text']\n",
    "#     tokens = tokenizer.tokenize(text.lower())\n",
    "#     #stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#     #texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "#     #texts = [i for i in texts if len(i) > 1]\n",
    "#     processed_text = ' '.join(tokens)\n",
    "#     df_review0.loc[index,'text'] = processed_text\n",
    "#df_review.iloc[1000, :].text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "def clean_review(review):\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    words = letters_only.lower().split()\n",
    "    #stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    useful_words = [x for x in words if not x in en_stop]\n",
    "    useful_words1 = [x for x in useful_words if len(x)>1]\n",
    "    texts = [p_stemmer.stem(i) for i in useful_words1]\n",
    "    # Combine words into a paragraph again\n",
    "    useful_words_string = ' '.join(texts)\n",
    "    return(useful_words_string)\n",
    "\n",
    "start_time = time.time()\n",
    "df_review['text'] = df_review['text'].apply(clean_review)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "#df_review.to_csv('/Users/ting/Documents/CMPT741/project/train_reviewCLEAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693209, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_review.to_csv('/Users/ting/Documents/CMPT741/project/train_reviewCLEAN1.csv')\n",
    "user_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review.rename(columns={'id': 'train_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_join = df.join(df_review.set_index('train_id'), on = 'train_id')\n",
    "\n",
    "#build user_document\n",
    "df_user_text = df_join[['user_id','text']]\n",
    "df_user_text = df_user_text.groupby('user_id').apply(lambda x: x.sum())\n",
    "df_user_text.to_csv('/Users/ting/Documents/CMPT741/project/user_text.csv')\n",
    "\n",
    "#build business_document\n",
    "df_business_text = df_join[['business_id','text']]\n",
    "df_business_text = df_business_text.groupby('business_id').apply(lambda x: x.sum())\n",
    "df_business_text.to_csv('/Users/ting/Documents/CMPT741/project/business_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#df_text = pd.read_csv(\"/Users/ting/Documents/CMPT741/project/business_text.csv\")\n",
    "df_text = df_business_text\n",
    "n_features = 500\n",
    "corpus = df_text['text']\n",
    "vectorizer = TfidfVectorizer(max_features = n_features,\n",
    "                             stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "#vectorizer.get_feature_names()\n",
    "business = df_text['business_id'].unique()\n",
    "business_dict = {value:index for index,value in enumerate(business)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_text = df_user_text\n",
    "\n",
    "corpus = df_text['text']\n",
    "vectorizer = TfidfVectorizer(max_features = n_features,\n",
    "                             stop_words='english')\n",
    "U = vectorizer.fit_transform(corpus).toarray()\n",
    "#vectorizer.get_feature_names()\n",
    "user = df_text['user_id'].unique()\n",
    "user_dict = {value:index for index,value in enumerate(user)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_component = 10\n",
    "\n",
    "lda_b = LatentDirichletAllocation(n_components=n_component, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_b.fit(X)\n",
    "business_matrix = lda_b.transform(X)\n",
    "\n",
    "lda_u = LatentDirichletAllocation(n_components=n_component, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_u.fit(U)\n",
    "user_matrix = lda_u.transform(U)\n",
    "\n",
    "# def print_top_words(model, feature_names, n_top_words):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         message = \"Topic #%d: \" % topic_idx\n",
    "#         message += \" \".join([feature_names[i]\n",
    "#                              for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "#         print(message)\n",
    "#     print()\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# print_top_words(lda, feature_names, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/Users/ting/Documents/CMPT741/project/test_rating.txt')\n",
    "test_cols = ['test_id','user_id', 'business_id']\n",
    "df_test = df_test[test_cols]\n",
    "\n",
    "###\n",
    "feature_list = []\n",
    "for i in range(n_component*2):\n",
    "    n = 'feature_' + str(i)\n",
    "    feature_list.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildFeatures(df):\n",
    "    df_zeros_1 = pd.DataFrame(0, index=np.arange(len(df)), columns=feature_list[:n_component])\n",
    "    df_zeros_2 = pd.DataFrame(0, index=np.arange(len(df)), columns=feature_list[n_component:])\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        u_id = row['user_id']\n",
    "        if u_id in user_dict:\n",
    "            u_vect = user_dict[u_id]\n",
    "            #df_zeros_1.loc[index]=U[u_vect]\n",
    "            df_zeros_1.loc[index]=user_matrix[u_vect]\n",
    "\n",
    "        b_id = row['business_id']\n",
    "        if b_id in business_dict:\n",
    "            b_vect = business_dict[b_id]\n",
    "            #df_zeros_2.loc[index]=X[b_vect]\n",
    "            df_zeros_2.loc[index]=business_matrix[b_vect]\n",
    "    return pd.concat([df_zeros_1, df_zeros_2],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145303, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_zeros_1 = pd.DataFrame(0, index=np.arange(len(df)), columns=feature_list[:n_component])\n",
    "# df_zeros_2 = pd.DataFrame(0, index=np.arange(len(df)), columns=feature_list[n_component:])\n",
    "# for index, row in df.iterrows():\n",
    "#     u_id = row['user_id']\n",
    "#     if u_id in user_dict:\n",
    "#         u_vect = user_dict[u_id]\n",
    "#         #df_zeros_1.loc[index]=U[u_vect]\n",
    "#         df_zeros_1.loc[index]=business_matrix[u_vect]\n",
    "\n",
    "#     b_id = row['business_id']\n",
    "#     if b_id in business_dict:\n",
    "#         b_vect = business_dict[b_id]\n",
    "#         #df_zeros_2.loc[index]=X[b_vect]\n",
    "#         df_zeros_2.loc[index]=user_matrix[b_vect]\n",
    "business_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_X = buildFeatures(df)\n",
    "df_train_Y = df['rating']\n",
    "\n",
    "df_test_X = buildFeatures(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train_X, df_train_Y, \n",
    "                                                    test_size=0.33, random_state=42)\n",
    "regr = RandomForestRegressor(max_depth=8, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "meanSquaredError = mean_squared_error(y_test, y_pred)\n",
    "rootMeanSquaredError = sqrt(meanSquaredError)\n",
    "print(\"RMSE:\", rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
